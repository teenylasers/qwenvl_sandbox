# Colab-Optimized DPO Training Configuration for Qwen3-VL-4B
# Optimized for T4 GPU (16GB VRAM) with session recovery
# Usage: python scripts/train_dpo.py --config configs/colab_dpo_config_4b.yaml --sft_checkpoint <path>

# Model settings
model:
  name: "Qwen/Qwen3-VL-4B-Instruct"
  sft_checkpoint: null  # Set via --sft_checkpoint or manually
  use_lora: true
  use_4bit: true
  lora:
    r: 32
    alpha: 16
    dropout: 0.05
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj

# DPO settings
dpo:
  beta: 0.1  # KL penalty coefficient
  loss_type: "sigmoid"
  label_smoothing: 0.0

# Dataset settings
data:
  max_samples: 500  # Limited for Colab
  shuffle: true
  seed: 42

# Training settings
training:
  output_dir: "/content/drive/MyDrive/qwen3_vl_training/checkpoints/dpo"
  num_epochs: 1
  batch_size: 1
  gradient_accumulation_steps: 16
  learning_rate: 5.0e-7
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_length: 1024
  max_prompt_length: 512
  gradient_checkpointing: true

# Hardware settings
hardware:
  bf16: true
  dataloader_num_workers: 2

# Logging settings
logging:
  steps: 10
  save_steps: 100  # More frequent for DPO
  save_total_limit: 2
  report_to: "wandb"
  run_name: "qwen-vl-dpo-colab-4b"

# Debug settings
debug:
  enabled: false
  max_samples: 30
  max_steps: 15
