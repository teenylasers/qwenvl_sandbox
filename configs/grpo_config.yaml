# GRPO Training Configuration for Qwen3-VL
# Usage: python scripts/train_grpo.py --config configs/grpo_config.yaml

# Model settings
model:
  name: "Qwen/Qwen3-VL-4B-Instruct"
  sft_checkpoint: null  # Path to SFT/DPO checkpoint
  use_lora: true
  use_4bit: true
  lora:
    r: 64
    alpha: 16
    dropout: 0.05

# GRPO settings
grpo:
  num_generations: 4  # Samples per prompt for group comparison
  temperature: 0.7
  top_p: 0.9
  max_new_tokens: 256
  reward_type: "combined"  # spatial, counting, or combined

# Reward function weights (for combined reward)
reward:
  spatial:
    exact_match_weight: 0.5
    spatial_weight: 0.3
    format_weight: 0.2
  counting:
    exact_weight: 0.7
    close_weight: 0.3

# Dataset settings
data:
  datasets:
    - rlhfv  # Use RLHF-V for now (spatial dataset is placeholder)
  max_samples_per_dataset: null
  shuffle: true
  seed: 42

# Training settings
training:
  output_dir: "./outputs/grpo"
  num_epochs: 1
  batch_size: 1
  gradient_accumulation_steps: 16
  learning_rate: 1.0e-6
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_length: 2048
  gradient_checkpointing: true

# Hardware settings
hardware:
  bf16: true
  dataloader_num_workers: 4
  # vLLM settings (cloud only, requires CUDA)
  use_vllm: false
  vllm_gpu_memory_utilization: 0.8

# Logging settings
logging:
  steps: 10
  save_steps: 500
  save_total_limit: 3
  report_to: "wandb"
  run_name: "qwen-vl-grpo-spatial"

# Debug settings
debug:
  enabled: false
  max_samples: 30
  max_steps: 20
