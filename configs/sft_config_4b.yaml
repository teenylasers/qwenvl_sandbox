# SFT Training Configuration for Qwen3-VL-4B
# Usage: python scripts/train_sft.py --config configs/sft_config_4b.yaml

# Model settings
model:
  name: "Qwen/Qwen3-VL-4B-Instruct"
  use_lora: true
  use_4bit: true  # Set to false for MPS/CPU
  lora:
    r: 64
    alpha: 16
    dropout: 0.05
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj

# Dataset settings
data:
  datasets:
    - rlhfv    # RLHF-V chosen responses
    - pixmo    # PixMo-Cap dense captions
    - spatial  # SpatialVLM
    # - llava_instruct  # LLaVA-Instruct-150K (downloads COCO images at load time)
    # - pixmo_points    # PixMo-Points 2.38M counting/pointing (downloads images from URLs)
    # - sharegpt4v      # ShareGPT4V 102K GPT4V captions (downloads COCO images)
    # - pixmo_docs      # PixMo-Docs 255K chart/table/diagram QA (embedded images)
  max_samples_per_dataset: null  # Set to small number for testing
  image:
    # Limit image resolution to control vision token count per image.
    # Each 28x28 patch = 1 vision token. Defaults: min=256*28*28, max=1280*28*28.
    min_pixels: 200704   # 256 * 28 * 28
    max_pixels: 1003520  # 1280 * 28 * 28
  shuffle: true
  seed: 42

# Training settings
training:
  output_dir: "./outputs/sft"
  num_epochs: 3
  batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-5
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_seq_length: 4096
  gradient_checkpointing: true

# Hardware settings
hardware:
  bf16: true  # Set to false for MPS/CPU
  dataloader_num_workers: 4

# Logging settings
logging:
  steps: 10
  save_steps: 500
  save_total_limit: 3
  report_to: "wandb"  # Set to "none" to disable
  run_name: "qwen-vl-sft-spatial-4b"

# Debug settings (for local testing)
debug:
  enabled: false
  max_samples: 100
  max_steps: 50
