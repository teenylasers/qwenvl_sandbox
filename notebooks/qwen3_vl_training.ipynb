{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3-VL Training Pipeline on Google Colab\n",
    "\n",
    "This notebook provides training for Qwen3-VL with:\n",
    "- **SFT** (Supervised Fine-Tuning)\n",
    "- **DPO** (Direct Preference Optimization)\n",
    "- **GRPO** (Group Relative Policy Optimization)\n",
    "\n",
    "**Requirements:**\n",
    "- GPU runtime (T4 free tier, or V100/A100 Pro)\n",
    "- Google Drive for checkpoints (recommended)\n",
    "- Weights & Biases account (optional)\n",
    "\n",
    "## Quick Start\n",
    "1. Select GPU runtime: `Runtime` > `Change runtime type` > `T4 GPU`\n",
    "2. Run cells in order\n",
    "3. Monitor training in WandB (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and type\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be very slow.\")\n",
    "    print(\"Go to Runtime > Change runtime type > T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive\n",
    "\n",
    "Mounting Drive ensures checkpoints and datasets persist across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Define paths\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/qwen3_vl_training\"\n",
    "CHECKPOINTS_DIR = f\"{DRIVE_BASE}/checkpoints\"\n",
    "DATASETS_CACHE = f\"{DRIVE_BASE}/datasets_cache\"\n",
    "HF_CACHE = f\"{DRIVE_BASE}/hf_cache\"\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [CHECKPOINTS_DIR, DATASETS_CACHE, HF_CACHE]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Set HuggingFace cache to Drive for persistence\n",
    "os.environ[\"HF_HOME\"] = HF_CACHE\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = DATASETS_CACHE\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n",
    "\n",
    "print(f\"Checkpoints: {CHECKPOINTS_DIR}\")\n",
    "print(f\"Dataset cache: {DATASETS_CACHE}\")\n",
    "print(f\"HF cache: {HF_CACHE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone repository (modify URL to your fork if needed)\n",
    "REPO_URL = \"https://github.com/teenylasers/qwenvl_sandbox.git\"\n",
    "REPO_DIR = \"/content/qwenvl_sandbox\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "else:\n",
    "    print(f\"Repository already exists at {REPO_DIR}\")\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "%cd {REPO_DIR}\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -e \".[cloud]\" -q\n",
    "\n",
    "print(\"\\nInstallation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. WandB Authentication (Optional)\n",
    "\n",
    "Weights & Biases provides experiment tracking. You can:\n",
    "1. Add `WANDB_API_KEY` to Colab Secrets (Settings > Secrets)\n",
    "2. Login interactively below\n",
    "3. Skip and set `USE_WANDB = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "USE_WANDB = True  # Set to False to disable\n",
    "\n",
    "if USE_WANDB:\n",
    "    try:\n",
    "        # Method 1: Try Colab Secrets\n",
    "        from google.colab import userdata\n",
    "\n",
    "        wandb_key = userdata.get(\"WANDB_API_KEY\")\n",
    "        wandb.login(key=wandb_key)\n",
    "        print(\"Logged in via Colab Secrets\")\n",
    "    except Exception as e:\n",
    "        # Method 2: Interactive login\n",
    "        print(\"Colab Secret not found. Trying interactive login...\")\n",
    "        try:\n",
    "            wandb.login()\n",
    "            print(\"Logged in interactively\")\n",
    "        except Exception as e2:\n",
    "            print(f\"WandB login failed: {e2}\")\n",
    "            print(\"Set USE_WANDB = False to continue without logging\")\n",
    "            USE_WANDB = False\n",
    "else:\n",
    "    print(\"WandB disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration\n",
    "\n",
    "Adjust these settings based on your GPU and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Training Configuration {display-mode: \"form\"}\n\n# @markdown ### Training Stage\nTRAINING_STAGE = \"sft\"  # @param [\"sft\", \"dpo\", \"grpo\"]\n\n# @markdown ### Dataset Options\nMAX_SAMPLES = 1000  # @param {type:\"integer\"}\nDATASETS = [\"rlhfv\"]  # @param {type:\"raw\"}\n\n# @markdown ### Training Options\nNUM_EPOCHS = 1  # @param {type:\"integer\"}\nBATCH_SIZE = 1  # @param {type:\"integer\"}\nGRADIENT_ACCUMULATION = 8  # @param {type:\"integer\"}\nLEARNING_RATE = 2e-5  # @param {type:\"number\"}\n\n# @markdown ### Checkpointing\nSAVE_STEPS = 200  # @param {type:\"integer\"}\nRESUME_FROM_CHECKPOINT = \"\"  # @param {type:\"string\"}\n\n# @markdown ### Previous Stage Checkpoint (for DPO/GRPO)\nSFT_CHECKPOINT = \"\"  # @param {type:\"string\"}\n\nprint(f\"Stage: {TRAINING_STAGE}\")\nprint(f\"Samples: {MAX_SAMPLES}\")\nprint(f\"Epochs: {NUM_EPOCHS}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Configuration\n",
    "\n",
    "Creates an optimized config based on detected GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import yaml\nimport torch\n\n\ndef detect_gpu_tier():\n    \"\"\"Detect GPU tier from hardware.\"\"\"\n    if not torch.cuda.is_available():\n        return \"cpu\"\n    gpu_name = torch.cuda.get_device_name(0).lower()\n    if \"a100\" in gpu_name:\n        return \"a100\"\n    elif \"v100\" in gpu_name:\n        return \"v100\"\n    else:\n        return \"t4\"\n\n\nGPU_TIER = detect_gpu_tier()\nprint(f\"Detected GPU tier: {GPU_TIER}\")\n\n# GPU-specific optimizations for 2B model (default)\n# max_pixels controls vision token count per image (1 token per 28x28 patch).\n# Must be sized so vision tokens fit within max_seq_length with room for text.\nGPU_CONFIGS = {\n    \"t4\": {\"max_seq_length\": 2048, \"lora_r\": 16, \"grpo_generations\": 4,\n            \"min_pixels\": 128 * 28 * 28, \"max_pixels\": 512 * 28 * 28},\n    \"v100\": {\"max_seq_length\": 1536, \"lora_r\": 32, \"grpo_generations\": 4,\n             \"min_pixels\": 128 * 28 * 28, \"max_pixels\": 768 * 28 * 28},\n    \"a100\": {\"max_seq_length\": 2048, \"lora_r\": 32, \"grpo_generations\": 4,\n             \"min_pixels\": 256 * 28 * 28, \"max_pixels\": 1280 * 28 * 28},\n    \"cpu\": {\"max_seq_length\": 512, \"lora_r\": 8, \"grpo_generations\": 1,\n            \"min_pixels\": 128 * 28 * 28, \"max_pixels\": 256 * 28 * 28},\n}\n\ngpu_cfg = GPU_CONFIGS[GPU_TIER]\noutput_dir = f\"{CHECKPOINTS_DIR}/{TRAINING_STAGE}\"\n\n# Build config\nconfig = {\n    \"model\": {\n        \"name\": \"Qwen/Qwen3-VL-2B-Instruct\",\n        \"use_lora\": True,\n        \"use_4bit\": True,\n        \"lora\": {\n            \"r\": gpu_cfg[\"lora_r\"],\n            \"alpha\": 16,\n            \"dropout\": 0.05,\n        },\n    },\n    \"data\": {\n        \"datasets\": DATASETS,\n        \"max_samples_per_dataset\": MAX_SAMPLES,\n        \"image\": {\n            \"min_pixels\": gpu_cfg[\"min_pixels\"],\n            \"max_pixels\": gpu_cfg[\"max_pixels\"],\n        },\n        \"shuffle\": True,\n        \"seed\": 42,\n    },\n    \"training\": {\n        \"output_dir\": output_dir,\n        \"num_epochs\": NUM_EPOCHS,\n        \"batch_size\": BATCH_SIZE,\n        \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION,\n        \"learning_rate\": LEARNING_RATE,\n        \"warmup_ratio\": 0.1,\n        \"weight_decay\": 0.01,\n        \"max_seq_length\": gpu_cfg[\"max_seq_length\"],\n        \"gradient_checkpointing\": True,\n    },\n    \"hardware\": {\n        \"bf16\": GPU_TIER != \"cpu\",\n        \"dataloader_num_workers\": 2,\n    },\n    \"logging\": {\n        \"steps\": 10,\n        \"save_steps\": SAVE_STEPS,\n        \"save_total_limit\": 2,\n        \"report_to\": \"wandb\" if USE_WANDB else \"none\",\n        \"run_name\": f\"qwen-vl-{TRAINING_STAGE}-colab\",\n    },\n    \"debug\": {\n        \"enabled\": False,\n    },\n}\n\n# Stage-specific settings\nif TRAINING_STAGE == \"dpo\":\n    config[\"dpo\"] = {\"beta\": 0.1, \"loss_type\": \"sigmoid\"}\n    config[\"training\"][\"learning_rate\"] = 5e-7\n    if SFT_CHECKPOINT:\n        config[\"model\"][\"sft_checkpoint\"] = SFT_CHECKPOINT\nelif TRAINING_STAGE == \"grpo\":\n    config[\"grpo\"] = {\n        \"num_generations\": gpu_cfg[\"grpo_generations\"],\n        \"temperature\": 0.7,\n        \"top_p\": 0.9,\n        \"reward_type\": \"combined\",\n    }\n    config[\"hardware\"][\"use_vllm\"] = False\n    config[\"training\"][\"learning_rate\"] = 1e-6\n    if SFT_CHECKPOINT:\n        config[\"model\"][\"sft_checkpoint\"] = SFT_CHECKPOINT\n\n# Save config\nconfig_path = f\"/content/qwenvl_sandbox/configs/colab_{TRAINING_STAGE}_runtime.yaml\"\nwith open(config_path, \"w\") as f:\n    yaml.dump(config, f, default_flow_style=False)\n\nprint(f\"\\nConfig saved to: {config_path}\")\nprint(f\"Output directory: {output_dir}\")\nprint(f\"Image pixels: min={gpu_cfg['min_pixels']} (~{gpu_cfg['min_pixels'] // (28*28)} tokens), \"\n      f\"max={gpu_cfg['max_pixels']} (~{gpu_cfg['max_pixels'] // (28*28)} tokens)\")\nprint(f\"Max sequence length: {gpu_cfg['max_seq_length']}\")\nprint(\"\\nConfiguration:\")\nprint(yaml.dump(config, default_flow_style=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Datasets\n",
    "\n",
    "Datasets are cached to Google Drive for persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_str = \" \".join(DATASETS)\n",
    "!python scripts/download_datasets.py --datasets {datasets_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Build command\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    f\"scripts/train_{TRAINING_STAGE}.py\",\n",
    "    \"--config\",\n",
    "    config_path,\n",
    "]\n",
    "\n",
    "if not USE_WANDB:\n",
    "    cmd.append(\"--no_wandb\")\n",
    "\n",
    "if RESUME_FROM_CHECKPOINT:\n",
    "    cmd.extend([\"--resume_from_checkpoint\", RESUME_FROM_CHECKPOINT])\n",
    "\n",
    "print(f\"Running: {' '.join(cmd)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run training\n",
    "process = subprocess.Popen(\n",
    "    cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1\n",
    ")\n",
    "\n",
    "for line in iter(process.stdout.readline, \"\"):\n",
    "    print(line, end=\"\")\n",
    "\n",
    "process.wait()\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training completed with return code: {process.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Checkpoint Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def list_checkpoints(base_dir):\n",
    "    \"\"\"List all checkpoints in the directory.\"\"\"\n",
    "    pattern = os.path.join(base_dir, \"**/checkpoint-*\")\n",
    "    checkpoints = glob.glob(pattern, recursive=True)\n",
    "    return sorted(checkpoints)\n",
    "\n",
    "\n",
    "print(\"Available checkpoints:\")\n",
    "print(\"=\" * 50)\n",
    "for cp in list_checkpoints(CHECKPOINTS_DIR):\n",
    "    size = sum(\n",
    "        os.path.getsize(os.path.join(cp, f))\n",
    "        for f in os.listdir(cp)\n",
    "        if os.path.isfile(os.path.join(cp, f))\n",
    "    )\n",
    "    print(f\"  {cp}\")\n",
    "    print(f\"    Size: {size / 1e6:.1f} MB\")\n",
    "\n",
    "print(\"\\nTo resume, copy checkpoint path to RESUME_FROM_CHECKPOINT above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Session Resume Helper\n",
    "\n",
    "Run this cell after a session timeout to restore your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Session Resume Helper {display-mode: \"form\"}\n",
    "# @markdown Run this after a session timeout\n",
    "\n",
    "# Re-mount Drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "# Restore environment variables\n",
    "import os\n",
    "\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/qwen3_vl_training\"\n",
    "os.environ[\"HF_HOME\"] = f\"{DRIVE_BASE}/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = f\"{DRIVE_BASE}/datasets_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = f\"{DRIVE_BASE}/hf_cache\"\n",
    "\n",
    "# Check if repo exists, re-clone if needed\n",
    "REPO_DIR = \"/content/qwenvl_sandbox\"\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    print(\"Repository not found. Please run Cell 3 to clone it.\")\n",
    "else:\n",
    "    %cd {REPO_DIR}\n",
    "    !pip install -e \".[cloud]\" -q\n",
    "    print(\"Environment restored!\")\n",
    "\n",
    "# List available checkpoints\n",
    "import glob\n",
    "\n",
    "print(\"\\nAvailable checkpoints to resume from:\")\n",
    "for cp in sorted(glob.glob(f\"{DRIVE_BASE}/checkpoints/*/checkpoint-*\")):\n",
    "    print(f\"  {cp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "After SFT training:\n",
    "1. Set `TRAINING_STAGE = \"dpo\"`\n",
    "2. Set `SFT_CHECKPOINT` to your SFT checkpoint path\n",
    "3. Re-run cells 5-8\n",
    "\n",
    "After DPO training:\n",
    "1. Set `TRAINING_STAGE = \"grpo\"`\n",
    "2. Set `SFT_CHECKPOINT` to your DPO checkpoint path\n",
    "3. Re-run cells 5-8"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}